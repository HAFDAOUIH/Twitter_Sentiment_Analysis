{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a448b4b7-1ebf-414a-ac87-cc48782a07b7",
   "metadata": {},
   "source": [
    "we are working in a local development environment (not in a distributed Spark cluster) so we should imoprt findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249b07f-9e58-43e7-ad90-2a4322b94873",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f1b8a3-1f52-43df-8179-27e569f513a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover,StringIndexer\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5b3ba-e4c7-41bf-bf76-7afb0de5a150",
   "metadata": {},
   "source": [
    "# Variables de contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a567d9c-df72-4dfd-bf71-4ebc8d0205d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:57:28 WARN Utils: Your hostname, sasamg-HP-Laptop-15s-eq2xxx resolves to a loopback address: 127.0.1.1; using 192.168.100.236 instead (on interface wlo1)\n",
      "24/04/30 23:57:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/30 23:57:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/30 23:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/30 23:57:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark= SparkSession.builder.config(\"spark.storage.memoryFraction\", \"0.6\").appName('Twitter_NB').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af6f72a-09d7-4c50-b00f-38a136f4cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = 'twitter_training.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8e07a-5d43-472a-85d5-8970fe724940",
   "metadata": {},
   "source": [
    "### we defined a StructType. This allows us when reading the CSV containing the data, to tell Spark to load the data according to the schema defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e81e36d-14cd-491c-a093-17a19558760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"game\", StringType(), True),\n",
    "    StructField(\"sentiment\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19047210-1a9b-4c18-a346-b3fe3bd78f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- game: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.csv(training_path, inferSchema=True, schema = schema)\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c7f7b49-3622-4f1a-b8e8-8ef1699a406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------+--------------------+\n",
      "|  id|       game|sentiment|               tweet|\n",
      "+----+-----------+---------+--------------------+\n",
      "|2401|Borderlands| Positive|im getting on bor...|\n",
      "|2401|Borderlands| Positive|I am coming to th...|\n",
      "|2401|Borderlands| Positive|im getting on bor...|\n",
      "|2401|Borderlands| Positive|im coming on bord...|\n",
      "|2401|Borderlands| Positive|im getting on bor...|\n",
      "|2401|Borderlands| Positive|im getting into b...|\n",
      "|2402|Borderlands| Positive|So I spent a few ...|\n",
      "|2402|Borderlands| Positive|So I spent a coup...|\n",
      "|2402|Borderlands| Positive|So I spent a few ...|\n",
      "|2402|Borderlands| Positive|So I spent a few ...|\n",
      "|2402|Borderlands| Positive|2010 So I spent a...|\n",
      "|2402|Borderlands| Positive|                 was|\n",
      "|2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|2403|Borderlands|  Neutral|Rock-Hard La Vita...|\n",
      "|2403|Borderlands|  Neutral|Live Rock - Hard ...|\n",
      "|2403|Borderlands|  Neutral|I-Hard like me, R...|\n",
      "|2404|Borderlands| Positive|that was the firs...|\n",
      "|2404|Borderlands| Positive|this was the firs...|\n",
      "+----+-----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e34f05c-fd57-422b-8f96-705b830ce5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:57:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+---------------+----------+--------------------+\n",
      "|summary|               id|           game| sentiment|               tweet|\n",
      "+-------+-----------------+---------------+----------+--------------------+\n",
      "|  count|            74682|          74682|     74682|               73996|\n",
      "|   mean|6432.586165341046|           NULL|      NULL|                 3.2|\n",
      "| stddev|3740.427870177445|           NULL|      NULL|   2.007130147392398|\n",
      "|    min|                1|         Amazon|Irrelevant|                    |\n",
      "|    max|            13200|johnson&johnson|  Positive|ðŸ§» at Home Depot ...|\n",
      "+-------+-----------------+---------------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df4df98-8342-4205-9aa9-716c83ed5912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-----+\n",
      "| id|game|sentiment|tweet|\n",
      "+---+----+---------+-----+\n",
      "|  0|   0|        0|  686|\n",
      "+---+----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = dataset.select(*(spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in dataset.columns))\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9702c-1b54-40eb-8e33-601daaf8e4c7",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98431b51-015e-44f7-9c02-6564dc0565e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset contains 74682 samples.\n"
     ]
    }
   ],
   "source": [
    "print(\"The training dataset contains {} samples.\".format(dataset.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4803bbc-077e-4f85-9d25-2d8afbbac013",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(subset=[\"tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5019cb5-0544-4d3e-8f1d-4af8021843a5",
   "metadata": {},
   "source": [
    "#### The 'tweet' column in our dataset is in string format. Therefore, we cannot directly use it for training. First, we need to tokenize it, which we achieve with a tokenizer. Then, we convert these words into vectors using HashingTF. In the notebook's later part, we will see that CountVectorizer is used instead of this method. These two are completely separate methods, and both can be used. By applying these methods, we prepare our 'text' column for training by applying IDF to it. Finally, we label the target column with StringIndexer and convert it to double.\n",
    "\n",
    "    1. Tokinizer : text -> words\n",
    "    2. HashingTF : filtered_words -> tf\n",
    "    3. IDF : tf -> features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1360bd6b-bda8-40a8-9dc3-9d8a110495d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"TF\")\n",
    "idf = IDF(inputCol=\"TF\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fbd78-dc08-4120-a98e-80f20b7e94c8",
   "metadata": {},
   "source": [
    "#### now we will convert these categorical Target (column = sentiment) into numerical indices. we will use StringIndexer for this purpose.\n",
    "    4. IDF : StringIndexer : target -> label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9481e47-8fc8-4f5e-9fe7-5516a39e7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46f8a8-d883-46ca-9e49-3f938a16486a",
   "metadata": {},
   "source": [
    "### If we start with logistic regression, we can put all these pre-processing steps in a pipeline to make it easier to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f399770-71cd-4b81-841c-68b740574f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label, nb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e17b6f-dc90-45fb-b017-1efe248749c4",
   "metadata": {},
   "source": [
    "## Chargement du dataset et sÃ©paration train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34c111ad-4d56-4391-b6a2-fec84acd0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = dataset.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2f683-6d76-4fac-8681-68fe9aee4f5c",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "### Logistic Regression modelÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6556b20e-cacd-4634-b8ee-0ddd688f5e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:59:24 WARN DAGScheduler: Broadcasting large task binary with size 1938.7 KiB\n",
      "24/04/30 23:59:25 WARN DAGScheduler: Broadcasting large task binary with size 1923.3 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(train_set)\n",
    "predictions = pipeline_model.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cce6967-0bc5-4ce9-8c95-fd660b170e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "evaluator_weighted_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_weighted_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ac6d999-c8ca-45b5-b1ab-165eef4831c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:59:28 WARN DAGScheduler: Broadcasting large task binary with size 1651.5 KiB\n",
      "24/04/30 23:59:29 WARN DAGScheduler: Broadcasting large task binary with size 1639.8 KiB\n",
      "24/04/30 23:59:30 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/30 23:59:32 WARN DAGScheduler: Broadcasting large task binary with size 1651.5 KiB\n",
      "24/04/30 23:59:32 WARN DAGScheduler: Broadcasting large task binary with size 1639.8 KiB\n",
      "24/04/30 23:59:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/30 23:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1651.5 KiB\n",
      "24/04/30 23:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1639.8 KiB\n",
      "24/04/30 23:59:36 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/30 23:59:38 WARN DAGScheduler: Broadcasting large task binary with size 1643.6 KiB\n",
      "24/04/30 23:59:38 WARN DAGScheduler: Broadcasting large task binary with size 1631.9 KiB\n",
      "24/04/30 23:59:39 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/30 23:59:40 WARN DAGScheduler: Broadcasting large task binary with size 1643.6 KiB\n",
      "24/04/30 23:59:41 WARN DAGScheduler: Broadcasting large task binary with size 1631.9 KiB\n",
      "24/04/30 23:59:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/30 23:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1643.6 KiB\n",
      "24/04/30 23:59:44 WARN DAGScheduler: Broadcasting large task binary with size 1631.9 KiB\n",
      "24/04/30 23:59:44 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/30 23:59:46 WARN DAGScheduler: Broadcasting large task binary with size 1647.5 KiB\n",
      "24/04/30 23:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1635.8 KiB\n",
      "24/04/30 23:59:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/30 23:59:49 WARN DAGScheduler: Broadcasting large task binary with size 1647.5 KiB\n",
      "24/04/30 23:59:50 WARN DAGScheduler: Broadcasting large task binary with size 1635.8 KiB\n",
      "24/04/30 23:59:50 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/30 23:59:52 WARN DAGScheduler: Broadcasting large task binary with size 1647.5 KiB\n",
      "24/04/30 23:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1635.8 KiB\n",
      "24/04/30 23:59:53 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/04/30 23:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1938.7 KiB\n",
      "24/04/30 23:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1923.3 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator_accuracy,\n",
    "                          numFolds=3) \n",
    "\n",
    "cvModel = crossval.fit(train_set)\n",
    "predictions = cvModel.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ce8ec19-9f35-4752-a5cb-8a5460e5043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:59:57 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/04/30 23:59:57 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/04/30 23:59:58 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "24/04/30 23:59:58 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    }
   ],
   "source": [
    "accuracy_lr = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score_lr = evaluator_f1.evaluate(predictions)\n",
    "weighted_precision_lr = evaluator_weighted_precision.evaluate(predictions)\n",
    "weighted_recall_lr = evaluator_weighted_recall.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cd52e9d-6491-40a6-b574-7487e8e4e5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8411732531108984\n",
      "F1 Score: 0.8410991709322988\n",
      "Weighted Precision: 0.8429710973919331\n",
      "Weighted Recall: 0.8411732531108984\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_lr)\n",
    "print(\"F1 Score:\", f1_score_lr)\n",
    "print(\"Weighted Precision:\", weighted_precision_lr)\n",
    "print(\"Weighted Recall:\", weighted_recall_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3006c98-c57c-4add-a9f0-6a04fbb50b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 00:00:46 WARN TaskSetManager: Stage 166 contains a task of very large size (1370 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 00:00:47 WARN TaskSetManager: Stage 170 contains a task of very large size (1051 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 00:00:48 WARN TaskSetManager: Stage 178 contains a task of very large size (2095 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "pipeline_model.write().overwrite().save(\"logistique_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd669f9-70e3-4d63-bf45-b3de0bf238a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
