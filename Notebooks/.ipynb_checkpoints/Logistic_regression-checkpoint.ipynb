{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a448b4b7-1ebf-414a-ac87-cc48782a07b7",
   "metadata": {},
   "source": [
    "we are working in a local development environment (not in a distributed Spark cluster) so we should imoprt findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249b07f-9e58-43e7-ad90-2a4322b94873",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f1b8a3-1f52-43df-8179-27e569f513a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover,StringIndexer\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5b3ba-e4c7-41bf-bf76-7afb0de5a150",
   "metadata": {},
   "source": [
    "# Variables de contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a567d9c-df72-4dfd-bf71-4ebc8d0205d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:27:08 WARN Utils: Your hostname, hafdaoui-Vostro-3400 resolves to a loopback address: 127.0.1.1; using 192.168.1.105 instead (on interface wlp0s20f3)\n",
      "24/04/30 23:27:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/30 23:27:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark= SparkSession.builder.config(\"spark.storage.memoryFraction\", \"0.6\").appName('Twitter').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af6f72a-09d7-4c50-b00f-38a136f4cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = '/home/hafdaoui/Desktop/twitter_training.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8e07a-5d43-472a-85d5-8970fe724940",
   "metadata": {},
   "source": [
    "### we defined a StructType. This allows us when reading the CSV containing the data, to tell Spark to load the data according to the schema defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e81e36d-14cd-491c-a093-17a19558760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"game\", StringType(), True),\n",
    "    StructField(\"sentiment\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19047210-1a9b-4c18-a346-b3fe3bd78f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- game: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.csv(training_path, inferSchema=True, schema = schema)\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c7f7b49-3622-4f1a-b8e8-8ef1699a406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------+--------------------+\n",
      "|  id|       game|sentiment|               tweet|\n",
      "+----+-----------+---------+--------------------+\n",
      "|2401|Borderlands| Positive|im getting on bor...|\n",
      "|2401|Borderlands| Positive|I am coming to th...|\n",
      "|2401|Borderlands| Positive|im getting on bor...|\n",
      "|2401|Borderlands| Positive|im coming on bord...|\n",
      "|2401|Borderlands| Positive|im getting on bor...|\n",
      "|2401|Borderlands| Positive|im getting into b...|\n",
      "|2402|Borderlands| Positive|So I spent a few ...|\n",
      "|2402|Borderlands| Positive|So I spent a coup...|\n",
      "|2402|Borderlands| Positive|So I spent a few ...|\n",
      "|2402|Borderlands| Positive|So I spent a few ...|\n",
      "|2402|Borderlands| Positive|2010 So I spent a...|\n",
      "|2402|Borderlands| Positive|                 was|\n",
      "|2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|2403|Borderlands|  Neutral|Rock-Hard La Vita...|\n",
      "|2403|Borderlands|  Neutral|Live Rock - Hard ...|\n",
      "|2403|Borderlands|  Neutral|I-Hard like me, R...|\n",
      "|2404|Borderlands| Positive|that was the firs...|\n",
      "|2404|Borderlands| Positive|this was the firs...|\n",
      "+----+-----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e34f05c-fd57-422b-8f96-705b830ce5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:27:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+---------------+----------+--------------------+\n",
      "|summary|               id|           game| sentiment|               tweet|\n",
      "+-------+-----------------+---------------+----------+--------------------+\n",
      "|  count|            74682|          74682|     74682|               73996|\n",
      "|   mean|6432.586165341046|           null|      null|                 3.2|\n",
      "| stddev|3740.427870177445|           null|      null|   2.007130147392398|\n",
      "|    min|                1|         Amazon|Irrelevant|                    |\n",
      "|    max|            13200|johnson&johnson|  Positive|ðŸ§» at Home Depot ...|\n",
      "+-------+-----------------+---------------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df4df98-8342-4205-9aa9-716c83ed5912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-----+\n",
      "| id|game|sentiment|tweet|\n",
      "+---+----+---------+-----+\n",
      "|  0|   0|        0|  686|\n",
      "+---+----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = dataset.select(*(spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in dataset.columns))\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9702c-1b54-40eb-8e33-601daaf8e4c7",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98431b51-015e-44f7-9c02-6564dc0565e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset contains 74682 samples.\n"
     ]
    }
   ],
   "source": [
    "print(\"The training dataset contains {} samples.\".format(dataset.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4803bbc-077e-4f85-9d25-2d8afbbac013",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(subset=[\"tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5019cb5-0544-4d3e-8f1d-4af8021843a5",
   "metadata": {},
   "source": [
    "#### The 'tweet' column in our dataset is in string format. Therefore, we cannot directly use it for training. First, we need to tokenize it, which we achieve with a tokenizer. Then, we convert these words into vectors using HashingTF. In the notebook's later part, we will see that CountVectorizer is used instead of this method. These two are completely separate methods, and both can be used. By applying these methods, we prepare our 'text' column for training by applying IDF to it. Finally, we label the target column with StringIndexer and convert it to double.\n",
    "\n",
    "    1. Tokinizer : text -> words\n",
    "    2. HashingTF : filtered_words -> tf\n",
    "    3. IDF : tf -> features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1360bd6b-bda8-40a8-9dc3-9d8a110495d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"TF\")\n",
    "idf = IDF(inputCol=\"TF\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fbd78-dc08-4120-a98e-80f20b7e94c8",
   "metadata": {},
   "source": [
    "#### now we will convert these categorical Target (column = sentiment) into numerical indices. we will use StringIndexer for this purpose.\n",
    "    4. IDF : StringIndexer : target -> label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9481e47-8fc8-4f5e-9fe7-5516a39e7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "LR = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", family=\"multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46f8a8-d883-46ca-9e49-3f938a16486a",
   "metadata": {},
   "source": [
    "### If we start with logistic regression, we can put all these pre-processing steps in a pipeline to make it easier to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f399770-71cd-4b81-841c-68b740574f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, label, LR])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e17b6f-dc90-45fb-b017-1efe248749c4",
   "metadata": {},
   "source": [
    "## Chargement du dataset et sÃ©paration train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34c111ad-4d56-4391-b6a2-fec84acd0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = dataset.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2f683-6d76-4fac-8681-68fe9aee4f5c",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "### Logistic Regression modelÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6556b20e-cacd-4634-b8ee-0ddd688f5e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:27:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/04/30 23:27:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:35 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:36 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:38 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:39 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:41 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:43 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:45 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:47 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:50 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:51 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:52 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:53 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:54 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:56 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:59 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:27:59 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:00 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:01 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:21 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:23 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:26 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/04/30 23:28:29 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(train_set)\n",
    "predictions = pipeline_model.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cce6967-0bc5-4ce9-8c95-fd660b170e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "evaluator_weighted_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_weighted_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ce8ec19-9f35-4752-a5cb-8a5460e5043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:28:30 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/04/30 23:28:32 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/04/30 23:28:34 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/04/30 23:28:36 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "accuracy_lr = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score_lr = evaluator_f1.evaluate(predictions)\n",
    "weighted_precision_lr = evaluator_weighted_precision.evaluate(predictions)\n",
    "weighted_recall_lr = evaluator_weighted_recall.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cd52e9d-6491-40a6-b574-7487e8e4e5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8301654587720497\n",
      "F1 Score: 0.8300687317761439\n",
      "Weighted Precision: 0.8303081480098422\n",
      "Weighted Recall: 0.8301654587720497\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_lr)\n",
    "print(\"F1 Score:\", f1_score_lr)\n",
    "print(\"Weighted Precision:\", weighted_precision_lr)\n",
    "print(\"Weighted Recall:\", weighted_recall_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3006c98-c57c-4add-a9f0-6a04fbb50b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 23:28:39 WARN TaskSetManager: Stage 136 contains a task of very large size (4187 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/30 23:28:40 WARN TaskSetManager: Stage 144 contains a task of very large size (2794 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "pipeline_model.write().overwrite().save(\"logistique_pipeline.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
